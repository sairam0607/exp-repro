{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reproducible Experiments\n",
    "In this notebook, the aim is to show you how to build a reproducible experiment within Azure. In this example, we're going to train a MNIST classifier using Tensorflow and show you how you can follow some practices to make this work reproducible. Later you can use this framework and apply it to other ML problems.\n",
    "\n",
    "The reason we chose MNIST is to pick a very simple example as the main focus is to build a reproducible experiment and not to learn a new algorithm or to build a complex model.\n",
    "\n",
    "By the end of this repo, you'll be able to build reproducible experiments based on the below image.\n",
    "![Reproducible Experiments for Machine Learning](assets/reproducible_experiment_azure_machine_learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:**:\n",
    "\n",
    "In order to practice all parts of the following Notebook, you first need to get a free Azure credit. If you don't have it, you can simply obtain it through this link: https://azure.microsoft.com/en-us/free/\n",
    "\n",
    "You can run this notebook on your local latop, Azure Notebooks (notebooks.azure.com) or Notebook VMs:\n",
    "- Local Laptop - the following packages has to be installed:\n",
    "    - Azureml-SDK - with notebook,widget extensions\n",
    "    - tensorflow==1.13\n",
    "- Azure Notebooks:\n",
    "    - This is a free notebook, all of the packages for an ML experiment is installed\n",
    "- AzureML Notebook:\n",
    "    - This is a premium notebook that you can choose the VM type. Avoid using this feature for the workshop as you may burn your credit before the end or the workshop.\n",
    "\n",
    "Once you chose the execution environment, you need to create an Azure Machine Learning Service. Follow this instruction to build one:\n",
    "\n",
    "The following text is copied from: https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup#create-a-workspace\n",
    "\n",
    "\n",
    "An Azure Machine Learning workspace is a foundational resource in the cloud that you use to experiment, train, and deploy machine learning models. It ties your Azure subscription and resource group to an easily consumed object in the service.\n",
    "\n",
    "You create a workspace via the Azure portal, a web-based console for managing your Azure resources.\n",
    "\n",
    "1. Sign in to the Azure portal by using the credentials for the Azure subscription you use.\n",
    "1. In the upper-left corner of Azure portal, select + Create a resource.\n",
    "1. Create a new resource\n",
    "1. Use the search bar to find Machine Learning service workspace.\n",
    "1. Select Machine Learning service workspace.\n",
    "1. In the Machine Learning service workspace pane, select Create to begin.\n",
    "1. Provide the following information to configure your new workspace:\n",
    "    - **Field\tDescription**\n",
    "    - **Workspace name**: type in **FirstExample**.\n",
    "    - **Subscription**: Select the Azure subscription that you want to use. (Your free credit)\n",
    "    - **Resource group**: type in **MLOpsWorkshop**\n",
    "    - **Location**: type in **westus2**\n",
    "1. After you are finished configuring the workspace, select Create.\n",
    "When the process is finished, a deployment success message appears.\n",
    "1. To view the new workspace, select Go to resource.\n",
    "\n",
    "\n",
    "You can explore the resource from two view:\n",
    "1. https://portal.azure.com (you can access all resources including Azure ML)\n",
    "1. https://ml.azure.com (recently released - still in preview and dedicated to Azure ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download the MNIST sample files from Yann Lecun website to our development environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/mnist/test-labels.gz', <http.client.HTTPMessage at 0x1af757e1910>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.widgets import RunDetails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the AzureML SDK package to be able to communicate with Azure ML Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.49.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your subscription ID will be different replace the stirng with yours\n",
    "subscription_id = \"7427803d-97a5-4cb2-9973-974a20bf6aa5\"              #\"your_subscription_id\"\n",
    "resource_group =  \"MLOPsWorkshop\"                                     #\"your_resource_group\"\n",
    "workspace_name =  \"FirstExample\"                                      #\"your_workspace_name\"\n",
    "workspace_region = \"West US 2\"                                                   #\"your_workspace_region\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an object from Workspace class. the Workspace object will point to the created Workspace we created through the portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
     ]
    },
    {
     "ename": "PersistenceDecryptionError",
     "evalue": "[Errno 13] Decryption failed: [WinError 13] The data is invalid. App developer may consider this guidance: https://github.com/AzureAD/microsoft-authentication-extensions-for-python/wiki/PersistenceDecryptionError: 'C:\\\\Users\\\\qu299cr\\\\.azureml\\\\auth\\\\msal_token_cache.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal_extensions\\persistence.py:236\u001b[0m, in \u001b[0;36mFilePersistenceWithDataProtection.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dp_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munprotect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal_extensions\\windows.py:123\u001b[0m, in \u001b[0;36mWindowsDataProtectionAgent.unprotect\u001b[1;34m(self, cipher_text)\u001b[0m\n\u001b[0;32m    122\u001b[0m err_code \u001b[38;5;241m=\u001b[39m _GET_LAST_ERROR()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;28;01mNone\u001b[39;00m, _err_description\u001b[38;5;241m.\u001b[39mget(err_code), \u001b[38;5;28;01mNone\u001b[39;00m, err_code)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 13] The data is invalid",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPersistenceDecryptionError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import the Workspace class and check the azureml SDK version\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# exist_ok checks if workspace exists or not.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workspace\n\u001b[1;32m----> 6\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[43mWorkspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubscription_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m               \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ws\u001b[38;5;241m.\u001b[39mwrite_config()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\azureml\\core\\workspace.py:194\u001b[0m, in \u001b[0;36mWorkspace.__init__\u001b[1;34m(self, subscription_id, resource_group, workspace_name, auth, _location, _disable_service_check, _workspace_id, sku, tags, _cloud)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Class Workspace constructor to load an existing Azure Machine Learning Workspace.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m:param subscription_id: The Azure subscription ID containing the workspace.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m:type _cloud: str\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m auth:\n\u001b[1;32m--> 194\u001b[0m     auth \u001b[38;5;241m=\u001b[39m \u001b[43mInteractiveLoginAuthentication\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auth \u001b[38;5;241m=\u001b[39m auth\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription_id \u001b[38;5;241m=\u001b[39m subscription_id\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\azureml\\core\\authentication.py:571\u001b[0m, in \u001b[0;36mInteractiveLoginAuthentication.__init__\u001b[1;34m(self, force, tenant_id, cloud)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_to_login:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming interactive authentication. Please follow the instructions \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon the terminal.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 571\u001b[0m     \u001b[43mperform_interactive_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cloud_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInteractive authentication successfully completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\azureml\\_base_sdk_common\\common.py:573\u001b[0m, in \u001b[0;36mperform_interactive_login\u001b[1;34m(username, password, service_principal, tenant, allow_no_subscriptions, identity, use_device_code, use_cert_sn_issuer, cloud_type)\u001b[0m\n\u001b[0;32m    571\u001b[0m credential \u001b[38;5;241m=\u001b[39m ServicePrincipalAuth\u001b[38;5;241m.\u001b[39mbuild_credential(secret_or_certificate\u001b[38;5;241m=\u001b[39mpassword)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 573\u001b[0m     subscriptions \u001b[38;5;241m=\u001b[39m \u001b[43mprofile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minteractive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredential\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_principal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_device_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_device_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_no_subscriptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_no_subscriptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cert_sn_issuer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cert_sn_issuer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MsalError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# try polish unfriendly server errors\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m username:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\azureml\\_vendor\\azure_cli_core\\_profile.py:155\u001b[0m, in \u001b[0;36mProfile.login\u001b[1;34m(self, interactive, username, password, is_service_principal, tenant, scopes, use_device_code, allow_no_subscriptions, use_cert_sn_issuer, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m         user_identity \u001b[38;5;241m=\u001b[39m identity\u001b[38;5;241m.\u001b[39mlogin_with_device_code(scopes\u001b[38;5;241m=\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m         user_identity \u001b[38;5;241m=\u001b[39m \u001b[43midentity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin_with_auth_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_service_principal:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\azureml\\_vendor\\azure_cli_core\\auth\\identity.py:121\u001b[0m, in \u001b[0;36mIdentity.login_with_auth_code\u001b[1;34m(self, scopes, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m success_template, error_template \u001b[38;5;241m=\u001b[39m read_response_templates()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# For AAD, use port 0 to let the system choose arbitrary unused ephemeral port to avoid port collision\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# on port 8400 from the old design. However, ADFS only allows port 8400.\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsal_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire_token_interactive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselect_account\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8400\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_adfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuccess_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuccess_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m check_result(result)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\application.py:1819\u001b[0m, in \u001b[0;36mPublicClientApplication.acquire_token_interactive\u001b[1;34m(self, scopes, prompt, login_hint, domain_hint, claims_challenge, timeout, port, extra_scopes_to_consent, max_age, parent_window_handle, on_before_launching_ui, **kwargs)\u001b[0m\n\u001b[0;32m   1816\u001b[0m on_before_launching_ui(ui\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1817\u001b[0m telemetry_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_telemetry_context(\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mACQUIRE_TOKEN_INTERACTIVE)\n\u001b[1;32m-> 1819\u001b[0m response \u001b[38;5;241m=\u001b[39m _clean_up(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_token_by_browser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decorate_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_scope_to_consent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_scopes_to_consent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mredirect_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:\u001b[39;49m\u001b[38;5;132;43;01m{port}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Hardcode the host, for now. AAD portal rejects 127.0.0.1 anyway\u001b[39;49;00m\n\u001b[0;32m   1824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogin_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogin_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_age\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdomain_hint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclaims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtelemetry_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preferred_browser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1837\u001b[0m telemetry_context\u001b[38;5;241m.\u001b[39mupdate_telemetry(response)\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oidc.py:281\u001b[0m, in \u001b[0;36mClient.obtain_token_by_browser\u001b[1;34m(self, display, prompt, max_age, ui_locales, id_token_hint, login_hint, acr_values, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A native app can use this method to obtain token via a local browser.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03mInternally, it implements nonce to mitigate replay attack.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mfor descriptions on other parameters and return value.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m filtered_params \u001b[38;5;241m=\u001b[39m {k:v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    273\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(prompt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m prompt,\n\u001b[0;32m    274\u001b[0m     display\u001b[38;5;241m=\u001b[39mdisplay,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     acr_values\u001b[38;5;241m=\u001b[39macr_values,\n\u001b[0;32m    280\u001b[0m     )\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}  \u001b[38;5;66;03m# Filter out None values\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_token_by_browser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oauth2.py:639\u001b[0m, in \u001b[0;36mClient.obtain_token_by_browser\u001b[1;34m(self, redirect_uri, auth_code_receiver, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m         uri \u001b[38;5;241m=\u001b[39m redirect_uri \u001b[38;5;28;01mif\u001b[39;00m _redirect_uri\u001b[38;5;241m.\u001b[39mport \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m urlunparse((\n\u001b[0;32m    631\u001b[0m             _redirect_uri\u001b[38;5;241m.\u001b[39mscheme,\n\u001b[0;32m    632\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_redirect_uri\u001b[38;5;241m.\u001b[39mhostname, receiver\u001b[38;5;241m.\u001b[39mget_port()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m             _redirect_uri\u001b[38;5;241m.\u001b[39mfragment,\n\u001b[0;32m    637\u001b[0m             ))  \u001b[38;5;66;03m# It could be slightly different than raw redirect_uri\u001b[39;00m\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as redirect_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(uri))\n\u001b[1;32m--> 639\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obtain_token_by_browser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreceiver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredirect_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt listen on port \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. You may try port 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m listen_port)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oauth2.py:677\u001b[0m, in \u001b[0;36mClient._obtain_token_by_browser\u001b[1;34m(self, auth_code_receiver, scope, extra_scope_to_consent, redirect_uri, timeout, welcome_template, success_template, error_template, auth_params, auth_uri_callback, browser_name, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitiate_auth_code_flow(\n\u001b[0;32m    664\u001b[0m     redirect_uri\u001b[38;5;241m=\u001b[39mredirect_uri,\n\u001b[0;32m    665\u001b[0m     scope\u001b[38;5;241m=\u001b[39m_scope_set(scope) \u001b[38;5;241m|\u001b[39m _scope_set(extra_scope_to_consent),\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(auth_params \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[0;32m    667\u001b[0m auth_response \u001b[38;5;241m=\u001b[39m auth_code_receiver\u001b[38;5;241m.\u001b[39mget_auth_response(\n\u001b[0;32m    668\u001b[0m     auth_uri\u001b[38;5;241m=\u001b[39mflow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    669\u001b[0m     state\u001b[38;5;241m=\u001b[39mflow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Optional but we choose to do it upfront\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    675\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m    676\u001b[0m     )\n\u001b[1;32m--> 677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_token_by_auth_code_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\application.py:142\u001b[0m, in \u001b[0;36m_ClientWithCcsRoutingInfo.obtain_token_by_auth_code_flow\u001b[1;34m(self, auth_code_flow, auth_response, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m client_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m client_info:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Note: The value of X-AnchorMailbox is also case-insensitive\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-AnchorMailbox\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOid:\u001b[39m\u001b[38;5;132;01m{uid}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{utid}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_info)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ClientWithCcsRoutingInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_token_by_auth_code_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_code_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oidc.py:205\u001b[0m, in \u001b[0;36mClient.obtain_token_by_auth_code_flow\u001b[1;34m(self, auth_code_flow, auth_response, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobtain_token_by_auth_code_flow\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_code_flow, auth_response, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate the auth_response being redirected back, and then obtain tokens,\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    including ID token which can be used for user sign in.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    for descriptions on other parameters and return value.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_token_by_auth_code_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_code_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token_claims\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    208\u001b[0m         nonce_in_id_token \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token_claims\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oauth2.py:547\u001b[0m, in \u001b[0;36mClient.obtain_token_by_auth_code_flow\u001b[1;34m(self, auth_code_flow, auth_response, scope, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope must be None or a subset of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m auth_code_flow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# i.e. the first leg was successful\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obtain_token_by_authorization_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_code_flow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mredirect_uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Required, if \"redirect_uri\" parameter was included in the\u001b[39;49;00m\n\u001b[0;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# authorization request, and their values MUST be identical.\u001b[39;49;00m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mauth_code_flow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscope\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# It is both unnecessary and harmless, per RFC 6749.\u001b[39;49;00m\n\u001b[0;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# We use the same scope already used in auth request uri,\u001b[39;49;00m\n\u001b[0;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# thus token cache can know what scope the tokens are for.\u001b[39;49;00m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Extract and update the data\u001b[39;49;00m\n\u001b[0;32m    557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcode_verifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_code_flow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode_verifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# It means the first leg encountered error\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;66;03m# Here we do NOT return original auth_response as-is, to prevent a\u001b[39;00m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;66;03m# potential {..., \"access_token\": \"attacker's AT\"} input being leaked\u001b[39;00m\n\u001b[0;32m    564\u001b[0m     error \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: auth_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oauth2.py:731\u001b[0m, in \u001b[0;36mClient._obtain_token_by_authorization_code\u001b[1;34m(self, code, redirect_uri, scope, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_secret:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# client_id is required, if the client is not authenticating itself.\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# See https://tools.ietf.org/html/rfc6749#section-4.1.3\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id\n\u001b[1;32m--> 731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obtain_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthorization_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oidc.py:116\u001b[0m, in \u001b[0;36mClient._obtain_token\u001b[1;34m(self, grant_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_obtain_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, grant_type, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The result will also contain one more key \"id_token_claims\",\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    whose value will be a dictionary returned by :func:`~decode_id_token`.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obtain_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrant_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ret:\n\u001b[0;32m    118\u001b[0m         ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token_claims\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_id_token(ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\oauth2cli\\oauth2.py:788\u001b[0m, in \u001b[0;36mClient._obtain_token\u001b[1;34m(self, grant_type, params, data, also_save_rt, on_obtaining_tokens, *args, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;66;03m# but we represent it as a list which can be persisted to JSON\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# Note: The scope will generally be absent in authorization grant,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m#       but our obtain_token_by_authorization_code(...) encourages\u001b[39;00m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;66;03m#       app developer to still explicitly provide a scope here.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m         scope \u001b[38;5;241m=\u001b[39m _data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 788\u001b[0m     \u001b[43m(\u001b[49m\u001b[43mon_obtaining_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_obtaining_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscope\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrant_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrant_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# can be used to know an IdToken-less\u001b[39;49;00m\n\u001b[0;32m    793\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# response is for an app or for a user\u001b[39;49;00m\n\u001b[0;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\application.py:691\u001b[0m, in \u001b[0;36mClientApplication._build_client.<locals>.<lambda>\u001b[1;34m(event)\u001b[0m\n\u001b[0;32m    675\u001b[0m     default_body[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_secret\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m client_credential\n\u001b[0;32m    676\u001b[0m central_configuration \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthorization_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: authority\u001b[38;5;241m.\u001b[39mauthorization_endpoint,\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: authority\u001b[38;5;241m.\u001b[39mtoken_endpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m         urljoin(authority\u001b[38;5;241m.\u001b[39mtoken_endpoint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevicecode\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    682\u001b[0m     }\n\u001b[0;32m    683\u001b[0m central_client \u001b[38;5;241m=\u001b[39m _ClientWithCcsRoutingInfo(\n\u001b[0;32m    684\u001b[0m     central_configuration,\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id,\n\u001b[0;32m    686\u001b[0m     http_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client,\n\u001b[0;32m    687\u001b[0m     default_headers\u001b[38;5;241m=\u001b[39mdefault_headers,\n\u001b[0;32m    688\u001b[0m     default_body\u001b[38;5;241m=\u001b[39mdefault_body,\n\u001b[0;32m    689\u001b[0m     client_assertion\u001b[38;5;241m=\u001b[39mclient_assertion,\n\u001b[0;32m    690\u001b[0m     client_assertion_type\u001b[38;5;241m=\u001b[39mclient_assertion_type,\n\u001b[1;32m--> 691\u001b[0m     on_obtaining_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m event: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthority\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    693\u001b[0m     on_removing_rt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_cache\u001b[38;5;241m.\u001b[39mremove_rt,\n\u001b[0;32m    694\u001b[0m     on_updating_rt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_cache\u001b[38;5;241m.\u001b[39mupdate_rt)\n\u001b[0;32m    696\u001b[0m regional_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (client_credential  \u001b[38;5;66;03m# Currently regional endpoint only serves some CCA flows\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_regional_client):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\token_cache.py:317\u001b[0m, in \u001b[0;36mSerializableTokenCache.add\u001b[1;34m(self, event, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSerializableTokenCache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_state_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\token_cache.py:117\u001b[0m, in \u001b[0;36mTokenCache.add\u001b[1;34m(self, event, now)\u001b[0m\n\u001b[0;32m    114\u001b[0m wipe(event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[0;32m    115\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_secret\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massertion\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     wipe(event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}), (  \u001b[38;5;66;03m# These claims were useful during __add()\u001b[39;00m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token_claims\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Provided by broker\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal\\token_cache.py:192\u001b[0m, in \u001b[0;36mTokenCache.__add\u001b[1;34m(self, event, now)\u001b[0m\n\u001b[0;32m    190\u001b[0m         refresh_in \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh_in\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# It is an integer\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         at[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh_on\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(now \u001b[38;5;241m+\u001b[39m refresh_in)  \u001b[38;5;66;03m# Schema wants a string\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCredentialType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACCESS_TOKEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_account_creation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    195\u001b[0m     account \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_account_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: home_account_id,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment\u001b[39m\u001b[38;5;124m\"\u001b[39m: environment,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;66;03m# \"client_info\": response.get(\"client_info\"),  # Optional\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         }\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal_extensions\\token_cache.py:64\u001b[0m, in \u001b[0;36mPersistedTokenCache.modify\u001b[1;34m(self, credential_type, old_entry, new_key_value_pairs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify\u001b[39m(\u001b[38;5;28mself\u001b[39m, credential_type, old_entry, new_key_value_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CrossPlatLock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock_location):\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28msuper\u001b[39m(PersistedTokenCache, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodify(\n\u001b[0;32m     66\u001b[0m             credential_type,\n\u001b[0;32m     67\u001b[0m             old_entry,\n\u001b[0;32m     68\u001b[0m             new_key_value_pairs\u001b[38;5;241m=\u001b[39mnew_key_value_pairs)\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistence\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal_extensions\\token_cache.py:55\u001b[0m, in \u001b[0;36mPersistedTokenCache._reload_if_necessary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_sync \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistence\u001b[38;5;241m.\u001b[39mtime_last_modified():\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeserialize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_persistence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_sync \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PersistenceNotFound:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# From cache's perspective, a nonexistent persistence is a NO-OP.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\azureml\\Lib\\site-packages\\msal_extensions\\persistence.py:238\u001b[0m, in \u001b[0;36mFilePersistenceWithDataProtection.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dp_agent\u001b[38;5;241m.\u001b[39munprotect(data)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PersistenceDecryptionError(\n\u001b[0;32m    239\u001b[0m         err_no\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinerror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),  \u001b[38;5;66;03m# Exists in Python 3 on Windows\u001b[39;00m\n\u001b[0;32m    240\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecryption failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApp developer may consider this guidance: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/AzureAD/microsoft-authentication-extensions-for-python/wiki/PersistenceDecryptionError\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(exception),\n\u001b[0;32m    244\u001b[0m         location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_location,\n\u001b[0;32m    245\u001b[0m         )\n",
      "\u001b[1;31mPersistenceDecryptionError\u001b[0m: [Errno 13] Decryption failed: [WinError 13] The data is invalid. App developer may consider this guidance: https://github.com/AzureAD/microsoft-authentication-extensions-for-python/wiki/PersistenceDecryptionError: 'C:\\\\Users\\\\qu299cr\\\\.azureml\\\\auth\\\\msal_token_cache.bin'"
     ]
    }
   ],
   "source": [
    "# import the Workspace class and check the azureml SDK version\n",
    "# exist_ok checks if workspace exists or not.\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a new Experiment\n",
    "\n",
    "In software engineering world we have a new feature to develop. In ML/Data Science world, we work on experiments.\n",
    "\n",
    "**Experiments** represent the collection of trials used to validate a user's hypothesis. We call each trial a run.\n",
    "\n",
    "Here we create a new experiment, we want to make sure everything related to this experiment is saved within the workspace and the lineage between each artifact is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='at-manulife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Track and Log Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two ways to start a trail (Run). Interactively and through batch submite. The start_loggin() method is the interactive way of starting a trail. It returns a Run object that we can use to log important metrics or the trail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key metrics can a single value for the accuracy of an ML model, a list of values representing the distribution or the data or an image showing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing log tracking action by creating a Run object in the Experiment\n",
    "run =  exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see that the **azureml.git.repository_uri** is poinint to the remote repo and the **azureml.git.branch** property is poining to the active branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "# run = Run(experiment=exp, run_id='<get it from the result above>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunDetails** class helps you visualize the active run object. It creates a network connection with Azure ML Worspace to collect everything happening during the run. It gets updated every 15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logs** function will log a single-valued or multi-valued metric under the current RUN. There are several types of logs, metric, table, row, image, etc.\n",
    "\n",
    "Every time you add a new metric, check the widget above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some metrics about the input dataset:\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "# Unzipping the input dataset and conver the data points into Numpy arrays\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record how the input dataset looks like\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "    \n",
    "run.log_image(name='{}-samples-of-input-dataset'.format(sample_size), plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dist = pd.DataFrame(data=y_train, columns=['test_values'])['test_values'].value_counts()\n",
    "\n",
    "run.log_table('digit_dist', {\"count\":list(dist.values), \"digits\":list(dist.index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('train_dataset_size', X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('test_dataset_size', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the collected metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('metric_1: ', run.get_metrics('metric_1'))\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Datasets\n",
    "\n",
    "As an important part of experiment reproducibility, you'd like to separate your dataset from the training code and the development environment. There are two major ways that you can achieve this. First you can use datastores to define the connection to an Azure data store such as Blob, SQL or Databricks table, then you leverage Datasets to access the actual files and version the reference to those file assets.\n",
    "\n",
    "### 2.1 Generate data references for the ML job\n",
    "\n",
    "Once an ML Workspace is created, a storage account is created with a default container (a logical container that works as a folder - this is refered to as bucket in AWS S3). The container is attached to the Workspace automatically as the default storage account. You can find it as **workspaceblobstore** under Datastores (https://ml.azure.com). This storage account can be used for test and development but should not be used in production scenarios. Because if you decide to delete the Workspace, the default storage account is also deleted which results in losing your data. So it's wiser to create a separate storage account and attach it to the ML Workspace.\n",
    "\n",
    "Here is how to access the default storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_datastore returns the default datastore attached to the Workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name of the default datastore suggestion, it's a reference to a Blob storage. Blob storage is a general purpose data lake that can be used to store any type of binary, from image, to csv file. Here are other types of Azure Storage types that you can attach to the WorkSpace:\n",
    "\n",
    "- Azure File Share\n",
    "- Azure Data Lake\n",
    "- Azure Data Lake Gen2\n",
    "- Azure SQL Database\n",
    "- Azure PostgreSQL\n",
    "- Databricks File System\n",
    "\n",
    "The following is an example of registering (attaching) a new Blob storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.datastore import Datastore\n",
    "# blob_datastore = Datastore.register_azure_blob_container(\n",
    "#            workspace=ws,\n",
    "#            datastore_name='<datastore_name>',\n",
    "#            account_name='<account_name>', # Storage account name\n",
    "#            container_name='<container_name>', # Name of Azure blob container\n",
    "#            account_key='<account_key>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the files that I downloaded to this storage for longer retention. This datastore can be referenced later in my training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate Datasets\n",
    "\n",
    "There are two different types of Datasets, TabularDataset and FileDataset. The Tabular can be used to access tabular like datasources, such as csv, SQL, Databricks, etc which FileDatasets can be used for binary datasets such as image, audio, etc. The following is a way to define a tabular dataset from an online source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# create a TabularDataset from a delimited file behind a public web url\n",
    "web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path)\n",
    "\n",
    "# To convert the Dataset into Spark (you need to have Spark installed on your development environment)\n",
    "# titanic_ds.take(3).to_spark_dataframe()\n",
    "\n",
    "# preview the first 3 rows of titanic_ds\n",
    "titanic_ds.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are references to a datasource (registered under datasources or available over internet). In other words, they don't keep the data, they are the reference definitiosn. You can register the defined Datasets under the Datasets section (accessible through ml.azure.com). Each time you register the dataset under the same name, you'll get a new version generated. Later you can access a particular version for the registered Dataset. As the dataset doesn't store your data, if you remove or change the data, the Dataset object doesn't help you roll back the change. Therefore, it's recommended to keep the data untouched. \n",
    "\n",
    "In order to keep the actual dataset, you can copy the data (using Azure Data Factory SDK or AzCopy CLI tool or [Azure SDK](https://github.com/Azure/azure-sdk-for-python/tree/master/sdk/storage/azure-storage-blob)) to clone the data to a new blob location and keep the under a new Dataset object.\n",
    "\n",
    "In the following cells, I've generated two versions of the Dataset. Every version of the titanic_ds can have different reference structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time you can retrive a particular version of Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'titanic_ds', \n",
    "                                 version = 1)\n",
    "\n",
    "titanic_ds_v1.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample code to access Parquet files: link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets\n",
    "# \n",
    "# # create a TabularDataset with time series trait\n",
    "# datastore_paths = [(datastore, 'weather/*/*/*/data.parquet')]\n",
    "# \n",
    "# # get a coarse timestamp column from the path pattern\n",
    "# dataset = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format='weather/{coarse_time:yyy/MM/dd}/data.parquet')\n",
    "# \n",
    "# # set coarse timestamp to the virtual column created, and fine grain timestamp from a column in the data\n",
    "# dataset = dataset.with_timestamp_columns(fine_grain_timestamp='datetime', coarse_grain_timestamp='coarse_time')\n",
    "# \n",
    "# # filter with time-series-trait-specific methods\n",
    "# data_slice = dataset.time_before(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_after(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_between(datetime(2019, 1, 1), datetime(2019, 2, 1))\n",
    "# data_slice = dataset.time_recent(timedelta(weeks=1, days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going back to our MNIST problem, let's create a Dataset object from the file uploaded to the default Datastore and register the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(ds, 'mnist/test-images.gz'),\n",
    " (ds, 'mnist/test-labels.gz'),\n",
    " (ds, 'mnist/train-images.gz'),\n",
    " (ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "mnist_dataset = Dataset.File.from_files(datastore_paths)\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's register it within the Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define a new Dataset under **mnist_dataset** by accessing the actual file over the internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_paths = [\n",
    "            'http://lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "\n",
    "mnist_dataset_web = Dataset.File.from_files(path = web_paths)\n",
    "\n",
    "mnist_dataset_web.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'mnist_dataset', \n",
    "                                 version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of registered files back\n",
    "mnist_dataset_v1.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the level of flexibility, I create a different dataset for each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_path_test_images = [(ds, 'mnist/test-images.gz')]\n",
    "datastore_path_test_labels = [(ds, 'mnist/test-labels.gz')]\n",
    "datastore_path_train_images = [(ds, 'mnist/train-images.gz')]\n",
    "datastore_path_train_labels = [(ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "dataset_test_images = Dataset.File.from_files(datastore_path_test_images)\n",
    "dataset_test_images.register(workspace=ws, name='mnist_test_images', description='MNIST input dataset for test images', create_new_version = True)\n",
    "\n",
    "dataset_test_labels = Dataset.File.from_files(datastore_path_test_labels)\n",
    "dataset_test_labels.register(workspace=ws, name='mnist_test_labels', description='MNIST input dataset for test labels', create_new_version = True)\n",
    "\n",
    "dataset_train_images = Dataset.File.from_files(datastore_path_train_images)\n",
    "dataset_train_images.register(workspace=ws, name='mnist_train_images', description='MNIST input dataset for train images', create_new_version = True)\n",
    "\n",
    "dataset_train_labels = Dataset.File.from_files(datastore_path_train_labels)\n",
    "dataset_train_labels.register(workspace=ws, name='mnist_train_labels', description='MNIST input dataset for train labels', create_new_version = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the ml.azure.com portal to check whether the datasets are registered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m \"added datasets and versioned them\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train ML Model in a Reproducible Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Traditional way of running a experiment\n",
    "Now run a simple Tensorflow job here to train an image classifier base off of MNIST dataset. This example requires Tensorflow 1.13.\n",
    "\n",
    "This is a simple two later fully connected neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import load_data\n",
    "\n",
    "data_folder = os.path.join('data', 'mnist')\n",
    "\n",
    "print('training dataset is stored here:', data_folder)\n",
    "\n",
    "training_set_size = X_train.shape[0]\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    # construct the DNN\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "    h1 = tf.layers.dense(X, n_h1, activation=tf.nn.relu, name='h1')\n",
    "    h2 = tf.layers.dense(h1, n_h2, activation=tf.nn.relu, name='h2')\n",
    "    output = tf.layers.dense(h2, n_outputs, name='output')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Make this job more reproducible - Still not ideal\n",
    "\n",
    "A better way to train this NN model is to track the metrics (the better approach will be discussed later today)\n",
    "\n",
    "**Tracking the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='Train-TF-Locally')\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Hyper Parameters and Accuracy metrics for each epoch. Finally I collect the accuracy for the final epoch as the final accuracy metric.\n",
    "\n",
    "Then I upload the model into the output section of the Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_row('Hyper Parameters',\n",
    "        n_inputs=n_inputs,\n",
    "        n_h1=n_h1,\n",
    "        n_h2=n_h2,\n",
    "        n_outputs=n_outputs,\n",
    "        learning_rate=learning_rate,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_images = Dataset.get_by_name(workspace = ws, name = 'mnist_test_images')\n",
    "dataset_test_labels = Dataset.get_by_name(workspace = ws, name = 'mnist_test_labels')\n",
    "dataset_train_images = Dataset.get_by_name(workspace = ws, name = 'mnist_train_images')\n",
    "dataset_train_labels = Dataset.get_by_name(workspace = ws, name = 'mnist_train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_row('dataset_test_images', dataset_name='mnist_test_images', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_test_labels', dataset_name='mnist_test_labels', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_train_images', dataset_name='mnist_train_images', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_train_labels', dataset_name='mnist_train_labels', version=dataset_train_labels.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_images.download('data/from_dataset')\n",
    "dataset_test_labels.download('data/from_dataset')\n",
    "dataset_train_images.download('data/from_dataset')\n",
    "dataset_train_labels.download('data/from_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the latest version of registered Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_data('./data/from_dataset/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/from_dataset/train-labels.gz', True).reshape(-1)\n",
    "X_test = load_data('./data/from_dataset/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/from_dataset/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        run.log('accuracy-train', acc_train)\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "        run.log('accuracy-val', acc_val)\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    run.log('final-accuracy', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n",
    "    run.upload_folder('/outputs/model/', './outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all or one of the metrics:\n",
    "\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packaging the Local runs\n",
    "\n",
    "In this section, we'd like to improve the previous run and package the run in more managed way so that the environment is also tracked and versioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Editing a run configuration property on-fly.\n",
    "user_managed_env = Environment(\"local-mnist\")\n",
    "\n",
    "user_managed_env.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "user_managed_env.python.interpreter_path = r'C:\\Users\\hosarsha\\AppData\\Local\\Continuum\\miniconda3\\envs\\azureml-p7\\python'\n",
    "\n",
    "# conda_dep = CondaDependencies()\n",
    "# conda_dep.add_conda_package(\"scikit-learn\")\n",
    "# conda_dep.add_pip_package(\"pillow==5.4.1\")\n",
    "# user_managed_env.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist.py', \n",
    "                      arguments=['--data-folder', r'C:\\Users\\hosarsha\\OneDrive - Microsoft\\Projects\\Learning\\MLOps\\Day3\\exp-repro\\data',\n",
    "                                '--batch-size', 50,\n",
    "                                '--first-layer-neurons', 300,\n",
    "                                '--second-layer-neurons', 100,\n",
    "                                '--learning-rate', 0.01])\n",
    "src.run_config.environment = user_managed_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_managed_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.get_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 A better way to submit the TF job\n",
    "\n",
    "Instead of running the TF job on our local laptop (or notebooks.azure.com) there is an easier way to accomplish this task. You can:\n",
    "1. Create a remote compute - this can be a single VM, a set of VM (Azure ML Compute), Databricks or HDInsight Cluster (Spark).\n",
    "1. Instead of we dealing with the depandancies, we let Azure ML manage the dependencies. In other words, We tell it what is the type of script, whether it's TF, Pytorch or just a python script and we let Azure set up the create remote compute instance for us.\n",
    "1. We mount the dataset we created above to the compute node and we let Azure stream the data from the storage to the node\n",
    "1. We ask Azure to register the model into a Model management instead of the output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infra as Code: Creating a remote computer\n",
    "\n",
    "With a couple of lines, we create a GPU cluster ([NV6 \"1 NVIDIA Tesla M60\"](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu#nv-series)) of 1 node. (We set it to 1 as we don't want to burn the credit - in real production settings we go with much larger VM size and much more than 1 node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPU cluster of type NV6 with 1 node. (due to subscription's limitations we stick to 1 node)\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpuclusterNV\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "                                                           max_nodes=10,\n",
    "                                                           min_nodes=10)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AmlCompute.provisioning_configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmlCompute.provisioning_configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of compute targets\n",
    "\n",
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps from now:\n",
    "1. Create a train.py file which keeps contains the training code for the MNIST TF file\n",
    "    1. At the beginning of the train.py we define the arguments that we want to pass to the training file\n",
    "1. It has all of the other dependencies such as util.py\n",
    "1. We create a dictionary object that keeps all of the parameters need to be passed to the train.py\n",
    "1. We define an estimator object from azureml.train.dnn.TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the project-folder that keeps all of the files required to be passed to the remote node\n",
    "\n",
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"project-folder\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Providing the dataset as an input parameter\n",
    "\n",
    "One of the ways to make the training job flexible is to provide parameters instead of counting on hardcoded values. One of the important values is the data folder. As we've discussed the two ways of providing data to the ML Workspace (Datastores and Datasets) you can similarly provide both to your training job. In the example below, we have demostrated both method. As the Datasets requires us to install extra packages on the remote compute and this takes us longer to execute, we chose to use the Datastore for this example.\n",
    "\n",
    "The as_mount function connects the dataset or datastore to the target compute at the run time, makes it look like accessing the files as if an external storage is mounted to the remote compute. This is very efficient, as it doesn't bring un-accessed data into the remote compute.\n",
    "\n",
    "The other options is to call as_download function, but it requires the remote compute to fiest download the entire data within datastore or dataset before running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    # You can provide the path from the Data Store or\n",
    "    '--data-folder': ws.get_default_datastore().as_mount(),\n",
    "    # Provide the mount point to the latest version of the registered dataset. If you choose Dataset instead of Datastore,\n",
    "    # you need to install azureml-dataprep[fuse,pandas] on the compute node using pip_packages parameter \"pip_packages=['azureml-dataprep[fuse,pandas]']\"\n",
    "    # '--dataset-test-images': dataset_test_images.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-test-labels': dataset_test_labels.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-train-images': dataset_train_images.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-train-labels': dataset_train_labels.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 # pip_packages=['azureml-dataprep[fuse,pandas]'],\n",
    "                 # pip_packages = ['pandas', 'numpy', '...']\n",
    "                 # conda_packages = ['pandas', 'numpy', '...']\n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to **Tensorflow** class, you can submit Python script with **Pytorch**, **Chainer**, **SKLearn** and pure **PythonScript** classes to the remote **compute target**. Further reading: https://github.com/Azure/AzureML-Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Register the model into the Model Registry\n",
    "\n",
    "The Run object can access to the output folder that is saved under the current Run within the experiment. By using the register_model function, you can register the model under the Model tab of the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model(model_name='tf-dnn-mnist-single-run', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Environment Logging and Reproducibility\n",
    "\n",
    "When the script is submitted using an Estimator object, Azure ML deploys a GPU base Linux docker image on the remote compute and based on the pip_packages and conda_packages parameters you can reproduce the environment as you wish.\n",
    "\n",
    "Once the Run is submitted, the entire environment dependencies are logged and can be reproduced. Using the Run Details, you can explore the docker, conda and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly define the characteristics of the environment trhrough **RunConfiguration** class and provide it to the ScriptSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.runconfig import RunConfiguration\n",
    "# from azureml.core.conda_dependencies import CondaDependencies\n",
    "# from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "# \n",
    "# # create a new runconfig object\n",
    "# run_config = RunConfiguration()\n",
    "# \n",
    "# # enable Docker \n",
    "# run_config.environment.docker.enabled = True\n",
    "# \n",
    "# # set Docker base image to the default CPU-based image\n",
    "# run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# \n",
    "# # use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "# run_config.environment.python.user_managed_dependencies = False\n",
    "# \n",
    "# # specify CondaDependencies obj\n",
    "# run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "#                                                                                           'numpy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Hyperparameter tuning\n",
    "\n",
    "In most of our DS experiments, in order to get to the optimal model, you have to try several hyper parameters on your algorithm. This can become super slow and time consuming. Using HyperDrive namespace, you can provide several choices to be passed to your training script through the parameters. In the example below, I provided three choices for the batch-size, 5 choices for the first layer neurons, 4 choices for the second-layer-neurons and a random value from the continuous space of log uniform distribution for the learning-rate.\n",
    "\n",
    "Later by setting the max_total_runs as a parameter to the HyperDriveConfig, you can set the total number of tries. In the example below, it uses Random Sampling technique to find the next combination of hyperparameters to try. It will stop based on the BanditPolicy or if number of iterations reaches the max_total_runs.\n",
    "\n",
    "Here are the types of sampling the hyperparamer space:\n",
    "* Random sampling\n",
    "* Grid sampling\n",
    "* Bayesian sampling\n",
    "\n",
    "Link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters\n",
    "\n",
    "In case of having several nodes (in our case we only have 1 node), you can submit several parallel execution on multiple nodes by setting the max_concurrent_runs. For example, if you have set max_total_runs = 100 and set max_concurrent_runs to 20. Assuming that you have 20 nodes of GPU cluster, then there will be 20 concurrent runs on all of the 20 nodes. Therefore, in theory you require 5 cycles to compelete the entire 100 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n",
    "        '--second-layer-neurons': choice(10, 50, 200, 500),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params={'--data-folder': ws.get_default_datastore().as_mount()},\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveConfig(estimator=est, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='accuracy-val', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=4,\n",
    "                       max_concurrent_runs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "htr.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this run as you'll be having several models to collect. You can always get the best or collect the entire generated models. In the example below, the best model is retrieved and registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()\n",
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist-hyperp-tunning', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you like to delete the compute target - If you like to continue the tutorial, \n",
    "# please keep the compute target on.\n",
    "\n",
    "# compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m \"Added the Hyperparameter section\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
